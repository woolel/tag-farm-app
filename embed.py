import re
import duckdb
import torch
import gc  # [ì¶”ê°€] ë©”ëª¨ë¦¬ ì²­ì†Œìš©
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from typing import Dict, List, Any, Tuple

# [ì„¤ì • ìˆ˜ì •ë¨]
MODEL_NAME = 'BAAI/bge-m3'
DB_PATH = "farming_granular.duckdb"

# 8GB ë¨ ìƒì¡´ ì„¤ì •
BATCH_SIZE = 1           # [ì¤‘ìš”] í•œ ë²ˆì— í•˜ë‚˜ì”© ì²˜ë¦¬ (RAM í­ì¦ ë°©ì§€)
DB_INSERT_BATCH = 50     # DB ì €ì¥ì€ 50ê°œì”© ëª¨ì•„ì„œ
MAX_TEXT_LENGTH = 1536   # [íƒ€í˜‘] 2048 -> 1536 (ì•½ 25% ë¶€í•˜ ê°ì†Œ, ì—¬ì „íˆ ì¶©ë¶„íˆ ê¹€)

# [íƒœê·¸ ì‚¬ì „]
TAG_SETS = {
    "crop": ["ë²¼", "ë³´ë¦¬", "ë°€", "ì½©", "ì˜¥ìˆ˜ìˆ˜", "ê°ì", "ê³ êµ¬ë§ˆ", "ê³ ì¶”", "ë°°ì¶”", "ë¬´", "ë§ˆëŠ˜", "ì–‘íŒŒ", "ì˜¤ì´", "í† ë§ˆí† ", "ë”¸ê¸°", "ìˆ˜ë°•", "ë³µìˆ­ì•„", "ì‚¬ê³¼", "ë°°", "í¬ë„", "ê°", "ì¸ì‚¼", "ì˜¤ë¯¸ì", "ê¹¨", "ì†Œ", "ë¼ì§€", "ë‹­", "ê¿€ë²Œ"],
    "task": ["íŒŒì¢…", "ìœ¡ë¬˜", "ì •ì‹", "ì´ì•™", "ë¬¼ê´€ë¦¬", "ë¹„ë£Œ", "ì œì´ˆ", "ì „ì •", "ì ê³¼", "ë°©ì œ", "ìˆ˜í™•", "ê±´ì¡°", "ì €ì¥", "ì¢…ìì‹ ì²­", "ë°©ì—­", "ë†ê¸°ê³„ì ê²€", "ìš”ì•½"],
    "env": ["ê¸°ìƒì „ë§", "íƒœí’", "ì¥ë§ˆ", "ê°€ë­„", "í­ì—¼", "ë™í•´", "ëƒ‰í•´", "ì§‘ì¤‘í˜¸ìš°", "ì¼ì¡°ëŸ‰", "ì €ìˆ˜ìœ¨", "ì‹œì„¤í•˜ìš°ìŠ¤", "í™”ì¬ì˜ˆë°©", "ì›”ë™ê´€ë¦¬"],
    "pest": ["íƒ„ì €ë³‘", "ë„ì—´ë³‘", "í°ê°€ë£¨ë³‘", "ê³¼ìˆ˜í™”ìƒë³‘", "ì§„ë”§ë¬¼", "ì‘ì• ", "ì´ì±„ë²Œë ˆ", "ë©¸êµ¬", "êµ¬ì œì—­", "AI", "ASF"],
    "admin": ["PLS", "ë¹„ë£Œ", "ë³´ê¸‰ì¢…", "ì¬í•´ë³´í—˜", "ì‹œë²”ì‚¬ì—…", "ë†ì•½"]
}

# [ì •ê·œì‹ ì»´íŒŒì¼]
COMPILED_PATTERNS = {}
PARTICLES = "(?:ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì˜|ì™€|ê³¼|ë„|ë¡œ|ì—|ì„œ)?"

for category, tags in TAG_SETS.items():
    one_char_tags = [re.escape(tag) for tag in tags if len(tag) == 1]
    multi_char_tags = [re.escape(tag) for tag in tags if len(tag) > 1]
    patterns = []
    if one_char_tags:
        patterns.append(f"(?<![ê°€-í£])((?:{'|'.join(one_char_tags)})){PARTICLES}(?![ê°€-í£])")
    if multi_char_tags:
        patterns.append(f"((?:{'|'.join(multi_char_tags)}))")
    if patterns:
        COMPILED_PATTERNS[category] = re.compile("|".join(patterns))
    else:
        COMPILED_PATTERNS[category] = None

def init_db(con: duckdb.DuckDBPyConnection, embedding_dim: int) -> None:
    try:
        con.execute("INSTALL vss; LOAD vss;") 
    except Exception as e:
        print(f"âš ï¸ VSS í™•ì¥ ë¡œë“œ ê²½ê³ : {e}")
    con.execute("CREATE SEQUENCE IF NOT EXISTS seq_id START 1;")
    con.execute(f"""
        CREATE TABLE IF NOT EXISTS farm_info (
            id INTEGER PRIMARY KEY DEFAULT nextval('seq_id'),
            year INTEGER, month INTEGER,
            title TEXT,
            tags_crop VARCHAR[], tags_task VARCHAR[], tags_env VARCHAR[],
            tags_pest VARCHAR[], tags_admin VARCHAR[],
            content_md TEXT,
            embedding FLOAT[{embedding_dim}]
        )
    """)

def extract_smart_tags_optimized(text: str) -> Dict[str, List[str]]:
    extracted = {}
    for category, pattern in COMPILED_PATTERNS.items():
        if pattern:
            matches = pattern.findall(text)
            cleaned_matches = {next(filter(None, match), '') for match in matches if match}
            if '' in cleaned_matches: cleaned_matches.remove('')
            extracted[category] = sorted(list(cleaned_matches))
        else:
            extracted[category] = []
    return extracted

def clean_markdown(text: str) -> str:
    text = re.sub(r'\[.*?\]\(.*?\)', ' ', text)
    text = re.sub(r'[\|\-]', ' ', text) # í‘œ ê¸°í˜¸ ì œê±°
    text = re.sub(r'[#*`>]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def flush_buffer_to_db(con: duckdb.DuckDBPyConnection, buffer: List[Tuple]) -> None:
    if not buffer: return
    try:
        con.executemany("""
            INSERT INTO farm_info (year, month, title, tags_crop, tags_task, tags_env, tags_pest, tags_admin, content_md, embedding)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, buffer)
    except duckdb.Error as e:
        print(f"âŒ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def build_database(md_file_path: str):
    print("ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘... (BGE-M3)")
    model = SentenceTransformer(MODEL_NAME, device='cpu')
    
    print("âš¡ ëª¨ë¸ ì–‘ìí™” ì ìš© ì¤‘...")
    model = torch.quantization.quantize_dynamic(
        model, {torch.nn.Linear}, dtype=torch.qint8
    )
    
    embedding_dimension = model.get_sentence_embedding_dimension()
    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ì°¨ì›: {embedding_dimension})")

    con = duckdb.connect(DB_PATH)
    init_db(con, embedding_dimension)

    try:
        with open(md_file_path, 'r', encoding='utf-8') as f:
            data = f.read()
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {md_file_path}")
        return

    raw_sections = re.split(r'\n#\s*(?=\[)', data)
    
    buffer_rows = []
    batch_texts = []
    batch_meta = []
    
    print("ğŸ”„ ë°ì´í„° ì²˜ë¦¬ ë° ì„ë² ë”© ì‹œì‘ (ì•ˆì „ ëª¨ë“œ)...")
    
    for section in tqdm(raw_sections):
        if not section.strip(): continue
        
        lines = section.strip().split('\n')
        header = lines[0]
        if not header.startswith('#'): header = '# ' + header
        body = "\n".join(lines[1:])

        if "ëª© ì°¨" in header: continue
        
        date_match = re.search(r'\[(\d{4})-(\d{2})', header)
        if not date_match: continue
        year, month = int(date_match.group(1)), int(date_match.group(2))
        
        clean_body = clean_markdown(body)
        full_text = (clean_markdown(header) + ". " + clean_body)[:MAX_TEXT_LENGTH]
        
        search_range = header + " " + body[:1000]
        tags = extract_smart_tags_optimized(search_range)
        
        batch_texts.append(full_text)
        batch_meta.append({
            'year': year, 'month': month, 'title': header,
            'tags': tags, 'content': body
        })
        
        # BATCH_SIZE = 1 ì´ë¯€ë¡œ ë§¤ë²ˆ ì‹¤í–‰ë¨
        if len(batch_texts) >= BATCH_SIZE:
            try:
                embeddings = model.encode(batch_texts, show_progress_bar=False, batch_size=BATCH_SIZE)
                for meta, emb in zip(batch_meta, embeddings):
                    buffer_rows.append((
                        meta['year'], meta['month'], meta['title'],
                        meta['tags']['crop'], meta['tags']['task'], meta['tags']['env'],
                        meta['tags']['pest'], meta['tags']['admin'],
                        meta['content'], emb.tolist()
                    ))
            except Exception as e:
                print(f"âš ï¸ ì„ë² ë”© ì˜¤ë¥˜: {e}")
            finally:
                batch_texts = []
                batch_meta = []
        
        if len(buffer_rows) >= DB_INSERT_BATCH:
            flush_buffer_to_db(con, buffer_rows)
            buffer_rows = []
            
        # [ì¤‘ìš”] ë°˜ë³µë§ˆë‹¤ ë©”ëª¨ë¦¬ ì²­ì†Œ
        gc.collect()

    if batch_texts:
        embeddings = model.encode(batch_texts, show_progress_bar=False, batch_size=BATCH_SIZE)
        for meta, emb in zip(batch_meta, embeddings):
            buffer_rows.append((
                meta['year'], meta['month'], meta['title'],
                meta['tags']['crop'], meta['tags']['task'], meta['tags']['env'],
                meta['tags']['pest'], meta['tags']['admin'],
                meta['content'], emb.tolist()
            ))

    if buffer_rows:
        flush_buffer_to_db(con, buffer_rows)

    print("â³ VSS ì¸ë±ìŠ¤ ìƒì„± ì¤‘... (HNSW)")
    try:
        con.execute("CREATE INDEX IF NOT EXISTS vss_idx ON farm_info USING HNSW (embedding);")
        print(f"ğŸš€ ì„±ê³µ: {DB_PATH} ìƒì„± ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")

    con.close()

if __name__ == "__main__":
    build_database("weekly.md")